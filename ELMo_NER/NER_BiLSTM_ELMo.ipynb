{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all levels of warnings and info which can be exist from Tensorflow\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Bidirectional, TimeDistributed\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = \"D:\\\\WordEmbeddings\\\\GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "def read_dataset(dataset):\n",
    "    # read line by line\n",
    "    with open(dataset) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        d = list(reader)\n",
    "    \n",
    "    tags = []\n",
    "    sentences = []\n",
    "    \n",
    "    tag = []\n",
    "    sentence = []\n",
    "    \n",
    "    for elem in d:\n",
    "        # means that empty line\n",
    "        if len(elem) == 0:\n",
    "            tags.append(tag)\n",
    "            sentences.append(sentence)\n",
    "            \n",
    "            tag = []\n",
    "            sentence = []\n",
    "        \n",
    "        else:\n",
    "            w = elem[1]\n",
    "            t = elem[0]\n",
    "            \n",
    "            if w not in string.punctuation:\n",
    "                tag.append(t)\n",
    "                sentence.append(w)               \n",
    "            \n",
    "    \n",
    "    # create pandas dataframe\n",
    "    df = pd.DataFrame(list(zip(sentences, tags)), \n",
    "               columns =['words', 'tags'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = read_dataset(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[what, movies, star, bruce, willis]</td>\n",
       "      <td>[O, O, O, B-ACTOR, I-ACTOR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[show, me, films, with, drew, barrymore, from,...</td>\n",
       "      <td>[O, O, O, O, B-ACTOR, I-ACTOR, O, O, B-YEAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[what, movies, starred, both, al, pacino, and,...</td>\n",
       "      <td>[O, O, O, O, B-ACTOR, I-ACTOR, O, B-ACTOR, I-A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[find, me, all, of, the, movies, that, starred...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-ACTOR, I-ACTOR, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[find, me, a, movie, with, a, quote, about, ba...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0                [what, movies, star, bruce, willis]   \n",
       "1  [show, me, films, with, drew, barrymore, from,...   \n",
       "2  [what, movies, starred, both, al, pacino, and,...   \n",
       "3  [find, me, all, of, the, movies, that, starred...   \n",
       "4  [find, me, a, movie, with, a, quote, about, ba...   \n",
       "\n",
       "                                                tags  \n",
       "0                        [O, O, O, B-ACTOR, I-ACTOR]  \n",
       "1       [O, O, O, O, B-ACTOR, I-ACTOR, O, O, B-YEAR]  \n",
       "2  [O, O, O, O, B-ACTOR, I-ACTOR, O, B-ACTOR, I-A...  \n",
       "3  [O, O, O, O, O, O, O, O, B-ACTOR, I-ACTOR, O, ...  \n",
       "4                  [O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tags(df):\n",
    "    words = set()\n",
    "    tags = set()\n",
    "\n",
    "    # for padding operation over sentence\n",
    "    words.add(\"ENDPAD\")\n",
    "    tags.add(np.nan)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # for each word in each row\n",
    "        for word in row[\"words\"]:\n",
    "            words.add(word)\n",
    "        for tag in row[\"tags\"]:\n",
    "            tags.add(tag)\n",
    "\n",
    "    words = list(words)\n",
    "    tags = list(tags)\n",
    "    \n",
    "    return words,tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = get_unique_tags(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6709"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ENDPAD\" in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6357\n",
      "var\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(words):\n",
    "    if e == \"what\":\n",
    "        print(i)\n",
    "        print(\"var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {words[idx] : idx for idx, elem in enumerate(words)}\n",
    "tag2idx = {tags[idx] : idx for idx, elem in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6709"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6357"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"what\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan: 0,\n",
       " 'I-ACTOR': 1,\n",
       " 'B-YEAR': 2,\n",
       " 'B-TRAILER': 3,\n",
       " 'B-REVIEW': 4,\n",
       " 'I-RATINGS_AVERAGE': 5,\n",
       " 'B-PLOT': 6,\n",
       " 'I-GENRE': 7,\n",
       " 'I-CHARACTER': 8,\n",
       " 'B-ACTOR': 9,\n",
       " 'I-DIRECTOR': 10,\n",
       " 'I-SONG': 11,\n",
       " 'B-RATINGS_AVERAGE': 12,\n",
       " 'I-RATING': 13,\n",
       " 'B-SONG': 14,\n",
       " 'I-TITLE': 15,\n",
       " 'O': 16,\n",
       " 'B-TITLE': 17,\n",
       " 'B-DIRECTOR': 18,\n",
       " 'B-GENRE': 19,\n",
       " 'I-PLOT': 20,\n",
       " 'I-REVIEW': 21,\n",
       " 'B-CHARACTER': 22,\n",
       " 'I-TRAILER': 23,\n",
       " 'B-RATING': 24,\n",
       " 'I-YEAR': 25}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into numerical form\n",
    "X = [[word2idx[word] for word in row_elem['words']] for index, row_elem in df.iterrows()]\n",
    "y = [[tag2idx[tag] for tag in row_elem['tags']] for index, row_elem in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6357"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"what\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_lenth = 0\n",
    "for sentence in X:\n",
    "    if len(sentence) > max_seq_lenth:\n",
    "        max_seq_lenth = len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_lenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6357, 860, 4392, 4169, 5545]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences for LSTM\n",
    "X = pad_sequences(X, maxlen=max_seq_lenth, dtype='int32', padding='post', value=word2idx[\"ENDPAD\"])\n",
    "y = pad_sequences(y, maxlen=max_seq_lenth, dtype='int32', padding='post', value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9774, 47)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9774, 47)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 16, 16,  9,  1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot conversion for labels\n",
    "y = to_categorical(y, num_classes=len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9774, 47, 26)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tfhub.dev/google/elmo/2\"\n",
    "elmo = hub.Module(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(8), Dimension(1024)])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just a random sentence\n",
    "x = [\"Roasxxx ants are a popular snack in Columbia\"]\n",
    "x2 = [\"Roasted ants are a popular snack in Columbia\"]\n",
    "\n",
    "# Extract ELMo features \n",
    "embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "embeddings2 = elmo(x2, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding layer\n",
    "EMBEDDING_DIM = 1024\n",
    "\n",
    "embedding_matrix = np.zeros((len(words), EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    try:\n",
    "        embedding_vector = elmo(word.lower(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        # assign normal noise vector instead of full zeros\n",
    "        noise = np.random.normal(0, 1, (1024,))\n",
    "        embedding_matrix[i] = noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Callback\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model_NER.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create model\n",
    "input_node = Input(shape=(max_seq_lenth,))\n",
    "embedding = Embedding(input_dim=len(words), output_dim=1024, input_length=max_seq_lenth, weights=[embedding_matrix], trainable = True)(input_node)\n",
    "rec_layer = Bidirectional(LSTM(50, return_sequences= True, recurrent_dropout=0.2))(embedding)\n",
    "output = TimeDistributed(Dense(len(tag2idx), activation=\"softmax\"))(rec_layer)\n",
    "\n",
    "NER = Model(input_node, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 47, 1024)          6870016   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 47, 100)           430000    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 47, 26)            2626      \n",
      "=================================================================\n",
      "Total params: 7,302,642\n",
      "Trainable params: 7,302,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NER.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1709 samples, validate on 733 samples\n",
      "Epoch 1/20\n",
      "1709/1709 [==============================] - ETA: 3:25 - loss: 3.2014 - acc: 0.002 - ETA: 1:47 - loss: 2.3243 - acc: 0.422 - ETA: 1:14 - loss: 1.8709 - acc: 0.574 - ETA: 57s - loss: 1.5844 - acc: 0.658 - ETA: 47s - loss: 1.3987 - acc: 0.70 - ETA: 40s - loss: 1.2676 - acc: 0.74 - ETA: 36s - loss: 1.1716 - acc: 0.76 - ETA: 32s - loss: 1.0868 - acc: 0.78 - ETA: 29s - loss: 1.0253 - acc: 0.79 - ETA: 26s - loss: 0.9766 - acc: 0.81 - ETA: 24s - loss: 0.9313 - acc: 0.82 - ETA: 23s - loss: 0.8934 - acc: 0.82 - ETA: 21s - loss: 0.8608 - acc: 0.83 - ETA: 20s - loss: 0.8291 - acc: 0.84 - ETA: 19s - loss: 0.7993 - acc: 0.84 - ETA: 18s - loss: 0.7788 - acc: 0.85 - ETA: 17s - loss: 0.7550 - acc: 0.85 - ETA: 16s - loss: 0.7342 - acc: 0.85 - ETA: 15s - loss: 0.7181 - acc: 0.86 - ETA: 14s - loss: 0.7033 - acc: 0.86 - ETA: 13s - loss: 0.6883 - acc: 0.86 - ETA: 13s - loss: 0.6755 - acc: 0.86 - ETA: 12s - loss: 0.6627 - acc: 0.87 - ETA: 12s - loss: 0.6487 - acc: 0.87 - ETA: 11s - loss: 0.6385 - acc: 0.87 - ETA: 10s - loss: 0.6274 - acc: 0.87 - ETA: 10s - loss: 0.6164 - acc: 0.87 - ETA: 9s - loss: 0.6051 - acc: 0.8809 - ETA: 9s - loss: 0.5967 - acc: 0.882 - ETA: 8s - loss: 0.5905 - acc: 0.882 - ETA: 8s - loss: 0.5847 - acc: 0.883 - ETA: 7s - loss: 0.5779 - acc: 0.884 - ETA: 7s - loss: 0.5699 - acc: 0.886 - ETA: 7s - loss: 0.5622 - acc: 0.887 - ETA: 6s - loss: 0.5564 - acc: 0.888 - ETA: 6s - loss: 0.5492 - acc: 0.889 - ETA: 5s - loss: 0.5425 - acc: 0.890 - ETA: 5s - loss: 0.5378 - acc: 0.891 - ETA: 5s - loss: 0.5309 - acc: 0.892 - ETA: 4s - loss: 0.5262 - acc: 0.893 - ETA: 4s - loss: 0.5212 - acc: 0.893 - ETA: 3s - loss: 0.5172 - acc: 0.894 - ETA: 3s - loss: 0.5132 - acc: 0.894 - ETA: 3s - loss: 0.5083 - acc: 0.895 - ETA: 2s - loss: 0.5033 - acc: 0.896 - ETA: 2s - loss: 0.4976 - acc: 0.897 - ETA: 2s - loss: 0.4938 - acc: 0.897 - ETA: 1s - loss: 0.4900 - acc: 0.898 - ETA: 1s - loss: 0.4865 - acc: 0.898 - ETA: 1s - loss: 0.4818 - acc: 0.899 - ETA: 0s - loss: 0.4780 - acc: 0.900 - ETA: 0s - loss: 0.4742 - acc: 0.901 - ETA: 0s - loss: 0.4718 - acc: 0.901 - 19s 11ms/step - loss: 0.4705 - acc: 0.9016 - val_loss: 0.3067 - val_acc: 0.9253\n",
      "Epoch 2/20\n",
      "1709/1709 [==============================] - ETA: 15s - loss: 0.2698 - acc: 0.93 - ETA: 14s - loss: 0.2718 - acc: 0.93 - ETA: 14s - loss: 0.2652 - acc: 0.93 - ETA: 14s - loss: 0.2667 - acc: 0.93 - ETA: 13s - loss: 0.2679 - acc: 0.93 - ETA: 13s - loss: 0.2799 - acc: 0.93 - ETA: 13s - loss: 0.2851 - acc: 0.93 - ETA: 12s - loss: 0.2819 - acc: 0.93 - ETA: 12s - loss: 0.2827 - acc: 0.93 - ETA: 12s - loss: 0.2775 - acc: 0.93 - ETA: 11s - loss: 0.2750 - acc: 0.93 - ETA: 11s - loss: 0.2801 - acc: 0.93 - ETA: 11s - loss: 0.2830 - acc: 0.93 - ETA: 10s - loss: 0.2840 - acc: 0.93 - ETA: 10s - loss: 0.2801 - acc: 0.93 - ETA: 10s - loss: 0.2805 - acc: 0.93 - ETA: 10s - loss: 0.2781 - acc: 0.93 - ETA: 9s - loss: 0.2748 - acc: 0.9331 - ETA: 9s - loss: 0.2718 - acc: 0.934 - ETA: 9s - loss: 0.2719 - acc: 0.933 - ETA: 8s - loss: 0.2728 - acc: 0.933 - ETA: 8s - loss: 0.2710 - acc: 0.934 - ETA: 8s - loss: 0.2709 - acc: 0.934 - ETA: 8s - loss: 0.2698 - acc: 0.934 - ETA: 7s - loss: 0.2689 - acc: 0.934 - ETA: 7s - loss: 0.2666 - acc: 0.934 - ETA: 7s - loss: 0.2661 - acc: 0.935 - ETA: 6s - loss: 0.2651 - acc: 0.935 - ETA: 6s - loss: 0.2646 - acc: 0.935 - ETA: 6s - loss: 0.2623 - acc: 0.935 - ETA: 6s - loss: 0.2640 - acc: 0.935 - ETA: 5s - loss: 0.2638 - acc: 0.935 - ETA: 5s - loss: 0.2628 - acc: 0.935 - ETA: 5s - loss: 0.2605 - acc: 0.935 - ETA: 5s - loss: 0.2599 - acc: 0.935 - ETA: 4s - loss: 0.2586 - acc: 0.936 - ETA: 4s - loss: 0.2568 - acc: 0.936 - ETA: 4s - loss: 0.2562 - acc: 0.936 - ETA: 3s - loss: 0.2551 - acc: 0.936 - ETA: 3s - loss: 0.2548 - acc: 0.937 - ETA: 3s - loss: 0.2535 - acc: 0.937 - ETA: 3s - loss: 0.2524 - acc: 0.937 - ETA: 2s - loss: 0.2517 - acc: 0.937 - ETA: 2s - loss: 0.2504 - acc: 0.938 - ETA: 2s - loss: 0.2498 - acc: 0.938 - ETA: 2s - loss: 0.2496 - acc: 0.938 - ETA: 1s - loss: 0.2492 - acc: 0.938 - ETA: 1s - loss: 0.2479 - acc: 0.938 - ETA: 1s - loss: 0.2474 - acc: 0.938 - ETA: 0s - loss: 0.2467 - acc: 0.938 - ETA: 0s - loss: 0.2463 - acc: 0.938 - ETA: 0s - loss: 0.2451 - acc: 0.939 - ETA: 0s - loss: 0.2447 - acc: 0.939 - 16s 9ms/step - loss: 0.2442 - acc: 0.9396 - val_loss: 0.2144 - val_acc: 0.9479\n",
      "Epoch 3/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.1755 - acc: 0.95 - ETA: 13s - loss: 0.1929 - acc: 0.95 - ETA: 13s - loss: 0.2000 - acc: 0.95 - ETA: 13s - loss: 0.1930 - acc: 0.95 - ETA: 12s - loss: 0.1956 - acc: 0.95 - ETA: 12s - loss: 0.1926 - acc: 0.95 - ETA: 12s - loss: 0.1908 - acc: 0.95 - ETA: 12s - loss: 0.1951 - acc: 0.95 - ETA: 11s - loss: 0.1960 - acc: 0.95 - ETA: 11s - loss: 0.1943 - acc: 0.95 - ETA: 11s - loss: 0.1953 - acc: 0.95 - ETA: 11s - loss: 0.1945 - acc: 0.95 - ETA: 10s - loss: 0.1950 - acc: 0.95 - ETA: 10s - loss: 0.1922 - acc: 0.95 - ETA: 10s - loss: 0.1927 - acc: 0.95 - ETA: 10s - loss: 0.1925 - acc: 0.95 - ETA: 9s - loss: 0.1927 - acc: 0.9531 - ETA: 9s - loss: 0.1923 - acc: 0.953 - ETA: 9s - loss: 0.1904 - acc: 0.953 - ETA: 9s - loss: 0.1900 - acc: 0.953 - ETA: 8s - loss: 0.1911 - acc: 0.953 - ETA: 8s - loss: 0.1906 - acc: 0.953 - ETA: 8s - loss: 0.1905 - acc: 0.953 - ETA: 8s - loss: 0.1895 - acc: 0.954 - ETA: 7s - loss: 0.1888 - acc: 0.954 - ETA: 7s - loss: 0.1888 - acc: 0.954 - ETA: 7s - loss: 0.1883 - acc: 0.954 - ETA: 6s - loss: 0.1863 - acc: 0.955 - ETA: 6s - loss: 0.1859 - acc: 0.955 - ETA: 6s - loss: 0.1852 - acc: 0.955 - ETA: 6s - loss: 0.1864 - acc: 0.955 - ETA: 5s - loss: 0.1872 - acc: 0.955 - ETA: 5s - loss: 0.1864 - acc: 0.955 - ETA: 5s - loss: 0.1853 - acc: 0.955 - ETA: 4s - loss: 0.1845 - acc: 0.955 - ETA: 4s - loss: 0.1843 - acc: 0.956 - ETA: 4s - loss: 0.1845 - acc: 0.956 - ETA: 4s - loss: 0.1851 - acc: 0.956 - ETA: 3s - loss: 0.1848 - acc: 0.956 - ETA: 3s - loss: 0.1844 - acc: 0.956 - ETA: 3s - loss: 0.1839 - acc: 0.956 - ETA: 3s - loss: 0.1829 - acc: 0.956 - ETA: 2s - loss: 0.1827 - acc: 0.957 - ETA: 2s - loss: 0.1820 - acc: 0.957 - ETA: 2s - loss: 0.1818 - acc: 0.957 - ETA: 1s - loss: 0.1813 - acc: 0.957 - ETA: 1s - loss: 0.1809 - acc: 0.957 - ETA: 1s - loss: 0.1812 - acc: 0.957 - ETA: 1s - loss: 0.1817 - acc: 0.957 - ETA: 0s - loss: 0.1813 - acc: 0.957 - ETA: 0s - loss: 0.1810 - acc: 0.957 - ETA: 0s - loss: 0.1804 - acc: 0.957 - ETA: 0s - loss: 0.1803 - acc: 0.957 - 16s 9ms/step - loss: 0.1807 - acc: 0.9578 - val_loss: 0.1701 - val_acc: 0.9596\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709/1709 [==============================] - ETA: 13s - loss: 0.1464 - acc: 0.97 - ETA: 13s - loss: 0.1588 - acc: 0.96 - ETA: 13s - loss: 0.1528 - acc: 0.97 - ETA: 13s - loss: 0.1560 - acc: 0.96 - ETA: 13s - loss: 0.1578 - acc: 0.96 - ETA: 13s - loss: 0.1522 - acc: 0.97 - ETA: 13s - loss: 0.1564 - acc: 0.96 - ETA: 12s - loss: 0.1543 - acc: 0.96 - ETA: 12s - loss: 0.1529 - acc: 0.96 - ETA: 12s - loss: 0.1517 - acc: 0.96 - ETA: 11s - loss: 0.1513 - acc: 0.96 - ETA: 11s - loss: 0.1516 - acc: 0.96 - ETA: 11s - loss: 0.1503 - acc: 0.96 - ETA: 11s - loss: 0.1482 - acc: 0.96 - ETA: 10s - loss: 0.1476 - acc: 0.96 - ETA: 10s - loss: 0.1473 - acc: 0.96 - ETA: 10s - loss: 0.1472 - acc: 0.96 - ETA: 9s - loss: 0.1456 - acc: 0.9695 - ETA: 9s - loss: 0.1458 - acc: 0.969 - ETA: 9s - loss: 0.1443 - acc: 0.969 - ETA: 9s - loss: 0.1431 - acc: 0.969 - ETA: 8s - loss: 0.1427 - acc: 0.969 - ETA: 8s - loss: 0.1421 - acc: 0.970 - ETA: 8s - loss: 0.1426 - acc: 0.969 - ETA: 7s - loss: 0.1415 - acc: 0.969 - ETA: 7s - loss: 0.1418 - acc: 0.969 - ETA: 7s - loss: 0.1415 - acc: 0.969 - ETA: 7s - loss: 0.1405 - acc: 0.969 - ETA: 6s - loss: 0.1401 - acc: 0.969 - ETA: 6s - loss: 0.1397 - acc: 0.969 - ETA: 6s - loss: 0.1396 - acc: 0.969 - ETA: 5s - loss: 0.1414 - acc: 0.969 - ETA: 5s - loss: 0.1417 - acc: 0.969 - ETA: 5s - loss: 0.1432 - acc: 0.968 - ETA: 5s - loss: 0.1435 - acc: 0.968 - ETA: 4s - loss: 0.1436 - acc: 0.968 - ETA: 4s - loss: 0.1435 - acc: 0.968 - ETA: 4s - loss: 0.1440 - acc: 0.968 - ETA: 3s - loss: 0.1459 - acc: 0.967 - ETA: 3s - loss: 0.1456 - acc: 0.967 - ETA: 3s - loss: 0.1446 - acc: 0.968 - ETA: 3s - loss: 0.1442 - acc: 0.968 - ETA: 2s - loss: 0.1443 - acc: 0.968 - ETA: 2s - loss: 0.1437 - acc: 0.968 - ETA: 2s - loss: 0.1435 - acc: 0.968 - ETA: 2s - loss: 0.1430 - acc: 0.968 - ETA: 1s - loss: 0.1427 - acc: 0.968 - ETA: 1s - loss: 0.1416 - acc: 0.968 - ETA: 1s - loss: 0.1416 - acc: 0.968 - ETA: 0s - loss: 0.1412 - acc: 0.968 - ETA: 0s - loss: 0.1408 - acc: 0.968 - ETA: 0s - loss: 0.1406 - acc: 0.968 - ETA: 0s - loss: 0.1404 - acc: 0.968 - 16s 10ms/step - loss: 0.1400 - acc: 0.9689 - val_loss: 0.1455 - val_acc: 0.9635\n",
      "Epoch 5/20\n",
      "1709/1709 [==============================] - ETA: 14s - loss: 0.1207 - acc: 0.97 - ETA: 13s - loss: 0.1234 - acc: 0.97 - ETA: 13s - loss: 0.1197 - acc: 0.97 - ETA: 13s - loss: 0.1174 - acc: 0.97 - ETA: 12s - loss: 0.1113 - acc: 0.97 - ETA: 12s - loss: 0.1073 - acc: 0.97 - ETA: 12s - loss: 0.1078 - acc: 0.97 - ETA: 12s - loss: 0.1067 - acc: 0.97 - ETA: 11s - loss: 0.1146 - acc: 0.97 - ETA: 11s - loss: 0.1151 - acc: 0.97 - ETA: 11s - loss: 0.1128 - acc: 0.97 - ETA: 10s - loss: 0.1140 - acc: 0.97 - ETA: 10s - loss: 0.1149 - acc: 0.97 - ETA: 10s - loss: 0.1152 - acc: 0.97 - ETA: 10s - loss: 0.1143 - acc: 0.97 - ETA: 9s - loss: 0.1149 - acc: 0.9746 - ETA: 9s - loss: 0.1163 - acc: 0.974 - ETA: 9s - loss: 0.1145 - acc: 0.974 - ETA: 9s - loss: 0.1138 - acc: 0.974 - ETA: 8s - loss: 0.1133 - acc: 0.974 - ETA: 8s - loss: 0.1127 - acc: 0.974 - ETA: 8s - loss: 0.1119 - acc: 0.974 - ETA: 8s - loss: 0.1116 - acc: 0.975 - ETA: 7s - loss: 0.1105 - acc: 0.975 - ETA: 7s - loss: 0.1107 - acc: 0.975 - ETA: 7s - loss: 0.1106 - acc: 0.975 - ETA: 6s - loss: 0.1113 - acc: 0.975 - ETA: 6s - loss: 0.1112 - acc: 0.974 - ETA: 6s - loss: 0.1110 - acc: 0.975 - ETA: 6s - loss: 0.1110 - acc: 0.975 - ETA: 5s - loss: 0.1113 - acc: 0.974 - ETA: 5s - loss: 0.1121 - acc: 0.974 - ETA: 5s - loss: 0.1124 - acc: 0.974 - ETA: 5s - loss: 0.1122 - acc: 0.974 - ETA: 4s - loss: 0.1122 - acc: 0.974 - ETA: 4s - loss: 0.1120 - acc: 0.974 - ETA: 4s - loss: 0.1121 - acc: 0.974 - ETA: 4s - loss: 0.1118 - acc: 0.974 - ETA: 3s - loss: 0.1126 - acc: 0.974 - ETA: 3s - loss: 0.1129 - acc: 0.974 - ETA: 3s - loss: 0.1134 - acc: 0.974 - ETA: 3s - loss: 0.1132 - acc: 0.974 - ETA: 2s - loss: 0.1126 - acc: 0.974 - ETA: 2s - loss: 0.1120 - acc: 0.974 - ETA: 2s - loss: 0.1119 - acc: 0.974 - ETA: 1s - loss: 0.1121 - acc: 0.974 - ETA: 1s - loss: 0.1121 - acc: 0.974 - ETA: 1s - loss: 0.1119 - acc: 0.974 - ETA: 1s - loss: 0.1116 - acc: 0.974 - ETA: 0s - loss: 0.1109 - acc: 0.974 - ETA: 0s - loss: 0.1105 - acc: 0.974 - ETA: 0s - loss: 0.1108 - acc: 0.974 - ETA: 0s - loss: 0.1110 - acc: 0.974 - 16s 9ms/step - loss: 0.1110 - acc: 0.9748 - val_loss: 0.1307 - val_acc: 0.9661\n",
      "Epoch 6/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0898 - acc: 0.98 - ETA: 13s - loss: 0.0933 - acc: 0.97 - ETA: 13s - loss: 0.0945 - acc: 0.97 - ETA: 12s - loss: 0.1001 - acc: 0.97 - ETA: 12s - loss: 0.1013 - acc: 0.97 - ETA: 12s - loss: 0.1038 - acc: 0.97 - ETA: 12s - loss: 0.1033 - acc: 0.97 - ETA: 11s - loss: 0.1023 - acc: 0.97 - ETA: 11s - loss: 0.1014 - acc: 0.97 - ETA: 11s - loss: 0.0984 - acc: 0.97 - ETA: 11s - loss: 0.0989 - acc: 0.97 - ETA: 10s - loss: 0.0990 - acc: 0.97 - ETA: 10s - loss: 0.0987 - acc: 0.97 - ETA: 10s - loss: 0.0981 - acc: 0.97 - ETA: 10s - loss: 0.0971 - acc: 0.97 - ETA: 9s - loss: 0.0965 - acc: 0.9786 - ETA: 9s - loss: 0.0963 - acc: 0.978 - ETA: 9s - loss: 0.0963 - acc: 0.978 - ETA: 9s - loss: 0.0949 - acc: 0.978 - ETA: 8s - loss: 0.0947 - acc: 0.979 - ETA: 8s - loss: 0.0942 - acc: 0.978 - ETA: 8s - loss: 0.0928 - acc: 0.979 - ETA: 7s - loss: 0.0914 - acc: 0.979 - ETA: 7s - loss: 0.0922 - acc: 0.979 - ETA: 7s - loss: 0.0924 - acc: 0.979 - ETA: 7s - loss: 0.0929 - acc: 0.979 - ETA: 6s - loss: 0.0929 - acc: 0.978 - ETA: 6s - loss: 0.0928 - acc: 0.978 - ETA: 6s - loss: 0.0930 - acc: 0.978 - ETA: 6s - loss: 0.0927 - acc: 0.978 - ETA: 5s - loss: 0.0921 - acc: 0.979 - ETA: 5s - loss: 0.0924 - acc: 0.979 - ETA: 5s - loss: 0.0923 - acc: 0.979 - ETA: 5s - loss: 0.0918 - acc: 0.979 - ETA: 4s - loss: 0.0916 - acc: 0.979 - ETA: 4s - loss: 0.0924 - acc: 0.979 - ETA: 4s - loss: 0.0924 - acc: 0.979 - ETA: 4s - loss: 0.0925 - acc: 0.979 - ETA: 3s - loss: 0.0930 - acc: 0.978 - ETA: 3s - loss: 0.0929 - acc: 0.979 - ETA: 3s - loss: 0.0929 - acc: 0.978 - ETA: 2s - loss: 0.0929 - acc: 0.978 - ETA: 2s - loss: 0.0927 - acc: 0.978 - ETA: 2s - loss: 0.0930 - acc: 0.978 - ETA: 2s - loss: 0.0926 - acc: 0.978 - ETA: 1s - loss: 0.0924 - acc: 0.978 - ETA: 1s - loss: 0.0920 - acc: 0.979 - ETA: 1s - loss: 0.0918 - acc: 0.978 - ETA: 1s - loss: 0.0917 - acc: 0.978 - ETA: 0s - loss: 0.0912 - acc: 0.979 - ETA: 0s - loss: 0.0910 - acc: 0.979 - ETA: 0s - loss: 0.0911 - acc: 0.979 - ETA: 0s - loss: 0.0906 - acc: 0.979 - 16s 9ms/step - loss: 0.0905 - acc: 0.9793 - val_loss: 0.1247 - val_acc: 0.9675\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0756 - acc: 0.98 - ETA: 13s - loss: 0.0674 - acc: 0.98 - ETA: 13s - loss: 0.0712 - acc: 0.98 - ETA: 12s - loss: 0.0816 - acc: 0.98 - ETA: 12s - loss: 0.0796 - acc: 0.98 - ETA: 12s - loss: 0.0760 - acc: 0.98 - ETA: 12s - loss: 0.0793 - acc: 0.98 - ETA: 11s - loss: 0.0805 - acc: 0.98 - ETA: 11s - loss: 0.0794 - acc: 0.98 - ETA: 11s - loss: 0.0803 - acc: 0.98 - ETA: 11s - loss: 0.0791 - acc: 0.98 - ETA: 10s - loss: 0.0816 - acc: 0.98 - ETA: 10s - loss: 0.0816 - acc: 0.98 - ETA: 10s - loss: 0.0806 - acc: 0.98 - ETA: 10s - loss: 0.0811 - acc: 0.98 - ETA: 9s - loss: 0.0801 - acc: 0.9814 - ETA: 9s - loss: 0.0810 - acc: 0.981 - ETA: 9s - loss: 0.0802 - acc: 0.981 - ETA: 9s - loss: 0.0797 - acc: 0.981 - ETA: 8s - loss: 0.0795 - acc: 0.981 - ETA: 8s - loss: 0.0794 - acc: 0.981 - ETA: 8s - loss: 0.0789 - acc: 0.982 - ETA: 7s - loss: 0.0781 - acc: 0.982 - ETA: 7s - loss: 0.0775 - acc: 0.982 - ETA: 7s - loss: 0.0771 - acc: 0.982 - ETA: 7s - loss: 0.0765 - acc: 0.982 - ETA: 6s - loss: 0.0762 - acc: 0.982 - ETA: 6s - loss: 0.0757 - acc: 0.982 - ETA: 6s - loss: 0.0757 - acc: 0.982 - ETA: 6s - loss: 0.0753 - acc: 0.982 - ETA: 5s - loss: 0.0756 - acc: 0.982 - ETA: 5s - loss: 0.0750 - acc: 0.982 - ETA: 5s - loss: 0.0750 - acc: 0.983 - ETA: 5s - loss: 0.0743 - acc: 0.983 - ETA: 4s - loss: 0.0738 - acc: 0.983 - ETA: 4s - loss: 0.0743 - acc: 0.983 - ETA: 4s - loss: 0.0738 - acc: 0.983 - ETA: 4s - loss: 0.0735 - acc: 0.983 - ETA: 3s - loss: 0.0737 - acc: 0.983 - ETA: 3s - loss: 0.0747 - acc: 0.983 - ETA: 3s - loss: 0.0744 - acc: 0.983 - ETA: 2s - loss: 0.0748 - acc: 0.983 - ETA: 2s - loss: 0.0748 - acc: 0.983 - ETA: 2s - loss: 0.0752 - acc: 0.983 - ETA: 2s - loss: 0.0749 - acc: 0.983 - ETA: 1s - loss: 0.0750 - acc: 0.983 - ETA: 1s - loss: 0.0755 - acc: 0.983 - ETA: 1s - loss: 0.0757 - acc: 0.982 - ETA: 1s - loss: 0.0757 - acc: 0.982 - ETA: 0s - loss: 0.0757 - acc: 0.982 - ETA: 0s - loss: 0.0755 - acc: 0.982 - ETA: 0s - loss: 0.0754 - acc: 0.982 - ETA: 0s - loss: 0.0753 - acc: 0.982 - 16s 9ms/step - loss: 0.0756 - acc: 0.9827 - val_loss: 0.1177 - val_acc: 0.9690\n",
      "Epoch 8/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0508 - acc: 0.98 - ETA: 13s - loss: 0.0554 - acc: 0.98 - ETA: 13s - loss: 0.0650 - acc: 0.98 - ETA: 12s - loss: 0.0622 - acc: 0.98 - ETA: 12s - loss: 0.0589 - acc: 0.98 - ETA: 12s - loss: 0.0603 - acc: 0.98 - ETA: 12s - loss: 0.0606 - acc: 0.98 - ETA: 11s - loss: 0.0619 - acc: 0.98 - ETA: 11s - loss: 0.0619 - acc: 0.98 - ETA: 11s - loss: 0.0627 - acc: 0.98 - ETA: 11s - loss: 0.0628 - acc: 0.98 - ETA: 10s - loss: 0.0622 - acc: 0.98 - ETA: 10s - loss: 0.0619 - acc: 0.98 - ETA: 10s - loss: 0.0635 - acc: 0.98 - ETA: 10s - loss: 0.0626 - acc: 0.98 - ETA: 9s - loss: 0.0632 - acc: 0.9862 - ETA: 9s - loss: 0.0663 - acc: 0.985 - ETA: 9s - loss: 0.0664 - acc: 0.985 - ETA: 9s - loss: 0.0656 - acc: 0.985 - ETA: 8s - loss: 0.0654 - acc: 0.985 - ETA: 8s - loss: 0.0649 - acc: 0.985 - ETA: 8s - loss: 0.0643 - acc: 0.986 - ETA: 7s - loss: 0.0638 - acc: 0.986 - ETA: 7s - loss: 0.0643 - acc: 0.986 - ETA: 7s - loss: 0.0648 - acc: 0.985 - ETA: 7s - loss: 0.0643 - acc: 0.985 - ETA: 6s - loss: 0.0642 - acc: 0.985 - ETA: 6s - loss: 0.0644 - acc: 0.985 - ETA: 6s - loss: 0.0644 - acc: 0.985 - ETA: 6s - loss: 0.0643 - acc: 0.985 - ETA: 5s - loss: 0.0638 - acc: 0.985 - ETA: 5s - loss: 0.0632 - acc: 0.986 - ETA: 5s - loss: 0.0639 - acc: 0.985 - ETA: 5s - loss: 0.0639 - acc: 0.985 - ETA: 4s - loss: 0.0650 - acc: 0.985 - ETA: 4s - loss: 0.0649 - acc: 0.985 - ETA: 4s - loss: 0.0657 - acc: 0.985 - ETA: 4s - loss: 0.0655 - acc: 0.985 - ETA: 3s - loss: 0.0654 - acc: 0.985 - ETA: 3s - loss: 0.0650 - acc: 0.985 - ETA: 3s - loss: 0.0648 - acc: 0.985 - ETA: 2s - loss: 0.0643 - acc: 0.985 - ETA: 2s - loss: 0.0644 - acc: 0.985 - ETA: 2s - loss: 0.0645 - acc: 0.985 - ETA: 2s - loss: 0.0649 - acc: 0.985 - ETA: 1s - loss: 0.0649 - acc: 0.985 - ETA: 1s - loss: 0.0647 - acc: 0.985 - ETA: 1s - loss: 0.0644 - acc: 0.985 - ETA: 1s - loss: 0.0640 - acc: 0.985 - ETA: 0s - loss: 0.0640 - acc: 0.985 - ETA: 0s - loss: 0.0638 - acc: 0.985 - ETA: 0s - loss: 0.0638 - acc: 0.985 - ETA: 0s - loss: 0.0637 - acc: 0.985 - 16s 9ms/step - loss: 0.0639 - acc: 0.9854 - val_loss: 0.1131 - val_acc: 0.9694\n",
      "Epoch 9/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0318 - acc: 0.99 - ETA: 13s - loss: 0.0378 - acc: 0.99 - ETA: 13s - loss: 0.0432 - acc: 0.99 - ETA: 12s - loss: 0.0473 - acc: 0.98 - ETA: 12s - loss: 0.0490 - acc: 0.98 - ETA: 12s - loss: 0.0489 - acc: 0.98 - ETA: 12s - loss: 0.0491 - acc: 0.98 - ETA: 11s - loss: 0.0491 - acc: 0.98 - ETA: 11s - loss: 0.0468 - acc: 0.98 - ETA: 11s - loss: 0.0484 - acc: 0.98 - ETA: 11s - loss: 0.0485 - acc: 0.98 - ETA: 10s - loss: 0.0500 - acc: 0.98 - ETA: 10s - loss: 0.0515 - acc: 0.98 - ETA: 10s - loss: 0.0517 - acc: 0.98 - ETA: 10s - loss: 0.0545 - acc: 0.98 - ETA: 9s - loss: 0.0531 - acc: 0.9881 - ETA: 9s - loss: 0.0537 - acc: 0.987 - ETA: 9s - loss: 0.0542 - acc: 0.987 - ETA: 8s - loss: 0.0545 - acc: 0.987 - ETA: 8s - loss: 0.0540 - acc: 0.987 - ETA: 8s - loss: 0.0541 - acc: 0.987 - ETA: 8s - loss: 0.0534 - acc: 0.987 - ETA: 7s - loss: 0.0535 - acc: 0.987 - ETA: 7s - loss: 0.0537 - acc: 0.987 - ETA: 7s - loss: 0.0536 - acc: 0.987 - ETA: 7s - loss: 0.0527 - acc: 0.988 - ETA: 6s - loss: 0.0533 - acc: 0.988 - ETA: 6s - loss: 0.0529 - acc: 0.988 - ETA: 6s - loss: 0.0526 - acc: 0.988 - ETA: 6s - loss: 0.0525 - acc: 0.988 - ETA: 5s - loss: 0.0531 - acc: 0.988 - ETA: 5s - loss: 0.0533 - acc: 0.988 - ETA: 5s - loss: 0.0531 - acc: 0.988 - ETA: 5s - loss: 0.0530 - acc: 0.988 - ETA: 4s - loss: 0.0528 - acc: 0.988 - ETA: 4s - loss: 0.0534 - acc: 0.987 - ETA: 4s - loss: 0.0535 - acc: 0.987 - ETA: 4s - loss: 0.0537 - acc: 0.987 - ETA: 3s - loss: 0.0540 - acc: 0.987 - ETA: 3s - loss: 0.0539 - acc: 0.987 - ETA: 3s - loss: 0.0545 - acc: 0.987 - ETA: 2s - loss: 0.0549 - acc: 0.987 - ETA: 2s - loss: 0.0554 - acc: 0.987 - ETA: 2s - loss: 0.0558 - acc: 0.987 - ETA: 2s - loss: 0.0557 - acc: 0.987 - ETA: 1s - loss: 0.0557 - acc: 0.987 - ETA: 1s - loss: 0.0556 - acc: 0.987 - ETA: 1s - loss: 0.0554 - acc: 0.987 - ETA: 1s - loss: 0.0553 - acc: 0.987 - ETA: 0s - loss: 0.0552 - acc: 0.987 - ETA: 0s - loss: 0.0551 - acc: 0.987 - ETA: 0s - loss: 0.0548 - acc: 0.987 - ETA: 0s - loss: 0.0547 - acc: 0.987 - 16s 9ms/step - loss: 0.0547 - acc: 0.9873 - val_loss: 0.1123 - val_acc: 0.9698\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0382 - acc: 0.99 - ETA: 13s - loss: 0.0398 - acc: 0.99 - ETA: 13s - loss: 0.0389 - acc: 0.99 - ETA: 12s - loss: 0.0385 - acc: 0.99 - ETA: 12s - loss: 0.0366 - acc: 0.99 - ETA: 12s - loss: 0.0398 - acc: 0.99 - ETA: 12s - loss: 0.0423 - acc: 0.99 - ETA: 11s - loss: 0.0424 - acc: 0.99 - ETA: 11s - loss: 0.0423 - acc: 0.99 - ETA: 11s - loss: 0.0443 - acc: 0.99 - ETA: 11s - loss: 0.0445 - acc: 0.99 - ETA: 10s - loss: 0.0435 - acc: 0.99 - ETA: 10s - loss: 0.0449 - acc: 0.98 - ETA: 10s - loss: 0.0447 - acc: 0.99 - ETA: 10s - loss: 0.0449 - acc: 0.99 - ETA: 9s - loss: 0.0449 - acc: 0.9899 - ETA: 9s - loss: 0.0457 - acc: 0.989 - ETA: 9s - loss: 0.0462 - acc: 0.989 - ETA: 9s - loss: 0.0464 - acc: 0.989 - ETA: 8s - loss: 0.0465 - acc: 0.989 - ETA: 8s - loss: 0.0459 - acc: 0.989 - ETA: 8s - loss: 0.0464 - acc: 0.989 - ETA: 7s - loss: 0.0466 - acc: 0.989 - ETA: 7s - loss: 0.0481 - acc: 0.989 - ETA: 7s - loss: 0.0476 - acc: 0.989 - ETA: 7s - loss: 0.0473 - acc: 0.989 - ETA: 6s - loss: 0.0475 - acc: 0.989 - ETA: 6s - loss: 0.0477 - acc: 0.989 - ETA: 6s - loss: 0.0475 - acc: 0.989 - ETA: 6s - loss: 0.0477 - acc: 0.989 - ETA: 5s - loss: 0.0474 - acc: 0.989 - ETA: 5s - loss: 0.0475 - acc: 0.989 - ETA: 5s - loss: 0.0475 - acc: 0.989 - ETA: 5s - loss: 0.0471 - acc: 0.989 - ETA: 4s - loss: 0.0470 - acc: 0.989 - ETA: 4s - loss: 0.0467 - acc: 0.989 - ETA: 4s - loss: 0.0465 - acc: 0.989 - ETA: 4s - loss: 0.0470 - acc: 0.989 - ETA: 3s - loss: 0.0474 - acc: 0.989 - ETA: 3s - loss: 0.0473 - acc: 0.989 - ETA: 3s - loss: 0.0470 - acc: 0.989 - ETA: 2s - loss: 0.0469 - acc: 0.989 - ETA: 2s - loss: 0.0471 - acc: 0.989 - ETA: 2s - loss: 0.0470 - acc: 0.989 - ETA: 2s - loss: 0.0472 - acc: 0.989 - ETA: 1s - loss: 0.0479 - acc: 0.988 - ETA: 1s - loss: 0.0478 - acc: 0.988 - ETA: 1s - loss: 0.0478 - acc: 0.988 - ETA: 1s - loss: 0.0476 - acc: 0.989 - ETA: 0s - loss: 0.0475 - acc: 0.989 - ETA: 0s - loss: 0.0476 - acc: 0.989 - ETA: 0s - loss: 0.0474 - acc: 0.989 - ETA: 0s - loss: 0.0473 - acc: 0.989 - 16s 9ms/step - loss: 0.0474 - acc: 0.9890 - val_loss: 0.1100 - val_acc: 0.9704\n",
      "Epoch 11/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0507 - acc: 0.98 - ETA: 13s - loss: 0.0429 - acc: 0.99 - ETA: 13s - loss: 0.0385 - acc: 0.99 - ETA: 12s - loss: 0.0373 - acc: 0.99 - ETA: 12s - loss: 0.0385 - acc: 0.99 - ETA: 12s - loss: 0.0392 - acc: 0.99 - ETA: 12s - loss: 0.0392 - acc: 0.99 - ETA: 11s - loss: 0.0389 - acc: 0.99 - ETA: 11s - loss: 0.0406 - acc: 0.99 - ETA: 11s - loss: 0.0407 - acc: 0.99 - ETA: 11s - loss: 0.0394 - acc: 0.99 - ETA: 10s - loss: 0.0418 - acc: 0.99 - ETA: 10s - loss: 0.0413 - acc: 0.99 - ETA: 10s - loss: 0.0418 - acc: 0.99 - ETA: 10s - loss: 0.0423 - acc: 0.99 - ETA: 9s - loss: 0.0422 - acc: 0.9906 - ETA: 9s - loss: 0.0416 - acc: 0.991 - ETA: 9s - loss: 0.0421 - acc: 0.990 - ETA: 8s - loss: 0.0422 - acc: 0.990 - ETA: 8s - loss: 0.0417 - acc: 0.990 - ETA: 8s - loss: 0.0417 - acc: 0.990 - ETA: 8s - loss: 0.0424 - acc: 0.990 - ETA: 7s - loss: 0.0428 - acc: 0.990 - ETA: 7s - loss: 0.0426 - acc: 0.990 - ETA: 7s - loss: 0.0433 - acc: 0.990 - ETA: 7s - loss: 0.0428 - acc: 0.990 - ETA: 6s - loss: 0.0427 - acc: 0.990 - ETA: 6s - loss: 0.0424 - acc: 0.990 - ETA: 6s - loss: 0.0422 - acc: 0.990 - ETA: 6s - loss: 0.0418 - acc: 0.990 - ETA: 5s - loss: 0.0416 - acc: 0.990 - ETA: 5s - loss: 0.0417 - acc: 0.990 - ETA: 5s - loss: 0.0417 - acc: 0.990 - ETA: 5s - loss: 0.0415 - acc: 0.990 - ETA: 4s - loss: 0.0416 - acc: 0.990 - ETA: 4s - loss: 0.0415 - acc: 0.990 - ETA: 4s - loss: 0.0416 - acc: 0.990 - ETA: 4s - loss: 0.0418 - acc: 0.990 - ETA: 3s - loss: 0.0421 - acc: 0.990 - ETA: 3s - loss: 0.0418 - acc: 0.990 - ETA: 3s - loss: 0.0417 - acc: 0.990 - ETA: 3s - loss: 0.0416 - acc: 0.990 - ETA: 2s - loss: 0.0414 - acc: 0.990 - ETA: 2s - loss: 0.0414 - acc: 0.990 - ETA: 2s - loss: 0.0416 - acc: 0.990 - ETA: 1s - loss: 0.0416 - acc: 0.990 - ETA: 1s - loss: 0.0417 - acc: 0.990 - ETA: 1s - loss: 0.0415 - acc: 0.990 - ETA: 1s - loss: 0.0413 - acc: 0.990 - ETA: 0s - loss: 0.0416 - acc: 0.990 - ETA: 0s - loss: 0.0417 - acc: 0.990 - ETA: 0s - loss: 0.0419 - acc: 0.990 - ETA: 0s - loss: 0.0417 - acc: 0.990 - 16s 9ms/step - loss: 0.0416 - acc: 0.9903 - val_loss: 0.1095 - val_acc: 0.9705\n",
      "Epoch 12/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0298 - acc: 0.99 - ETA: 13s - loss: 0.0296 - acc: 0.99 - ETA: 13s - loss: 0.0347 - acc: 0.99 - ETA: 13s - loss: 0.0341 - acc: 0.99 - ETA: 12s - loss: 0.0364 - acc: 0.99 - ETA: 12s - loss: 0.0359 - acc: 0.99 - ETA: 12s - loss: 0.0395 - acc: 0.99 - ETA: 12s - loss: 0.0396 - acc: 0.99 - ETA: 11s - loss: 0.0392 - acc: 0.99 - ETA: 11s - loss: 0.0395 - acc: 0.99 - ETA: 11s - loss: 0.0413 - acc: 0.99 - ETA: 11s - loss: 0.0399 - acc: 0.99 - ETA: 10s - loss: 0.0407 - acc: 0.99 - ETA: 10s - loss: 0.0410 - acc: 0.99 - ETA: 10s - loss: 0.0413 - acc: 0.99 - ETA: 10s - loss: 0.0413 - acc: 0.99 - ETA: 9s - loss: 0.0406 - acc: 0.9913 - ETA: 9s - loss: 0.0408 - acc: 0.991 - ETA: 9s - loss: 0.0409 - acc: 0.991 - ETA: 8s - loss: 0.0403 - acc: 0.991 - ETA: 8s - loss: 0.0398 - acc: 0.991 - ETA: 8s - loss: 0.0396 - acc: 0.991 - ETA: 8s - loss: 0.0396 - acc: 0.991 - ETA: 7s - loss: 0.0396 - acc: 0.991 - ETA: 7s - loss: 0.0392 - acc: 0.991 - ETA: 7s - loss: 0.0396 - acc: 0.991 - ETA: 7s - loss: 0.0393 - acc: 0.991 - ETA: 6s - loss: 0.0391 - acc: 0.991 - ETA: 6s - loss: 0.0386 - acc: 0.991 - ETA: 6s - loss: 0.0382 - acc: 0.991 - ETA: 5s - loss: 0.0378 - acc: 0.991 - ETA: 5s - loss: 0.0378 - acc: 0.991 - ETA: 5s - loss: 0.0376 - acc: 0.991 - ETA: 5s - loss: 0.0375 - acc: 0.991 - ETA: 4s - loss: 0.0380 - acc: 0.991 - ETA: 4s - loss: 0.0380 - acc: 0.991 - ETA: 4s - loss: 0.0377 - acc: 0.991 - ETA: 4s - loss: 0.0375 - acc: 0.991 - ETA: 3s - loss: 0.0374 - acc: 0.991 - ETA: 3s - loss: 0.0373 - acc: 0.991 - ETA: 3s - loss: 0.0371 - acc: 0.991 - ETA: 3s - loss: 0.0373 - acc: 0.991 - ETA: 2s - loss: 0.0373 - acc: 0.991 - ETA: 2s - loss: 0.0370 - acc: 0.991 - ETA: 2s - loss: 0.0369 - acc: 0.991 - ETA: 1s - loss: 0.0367 - acc: 0.991 - ETA: 1s - loss: 0.0367 - acc: 0.991 - ETA: 1s - loss: 0.0363 - acc: 0.991 - ETA: 1s - loss: 0.0362 - acc: 0.991 - ETA: 0s - loss: 0.0361 - acc: 0.991 - ETA: 0s - loss: 0.0363 - acc: 0.991 - ETA: 0s - loss: 0.0366 - acc: 0.991 - ETA: 0s - loss: 0.0365 - acc: 0.991 - 16s 9ms/step - loss: 0.0367 - acc: 0.9915 - val_loss: 0.1092 - val_acc: 0.9708\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0247 - acc: 0.99 - ETA: 13s - loss: 0.0228 - acc: 0.99 - ETA: 13s - loss: 0.0269 - acc: 0.99 - ETA: 12s - loss: 0.0309 - acc: 0.99 - ETA: 12s - loss: 0.0306 - acc: 0.99 - ETA: 12s - loss: 0.0300 - acc: 0.99 - ETA: 12s - loss: 0.0299 - acc: 0.99 - ETA: 11s - loss: 0.0296 - acc: 0.99 - ETA: 11s - loss: 0.0288 - acc: 0.99 - ETA: 11s - loss: 0.0289 - acc: 0.99 - ETA: 11s - loss: 0.0300 - acc: 0.99 - ETA: 10s - loss: 0.0288 - acc: 0.99 - ETA: 10s - loss: 0.0286 - acc: 0.99 - ETA: 10s - loss: 0.0286 - acc: 0.99 - ETA: 10s - loss: 0.0283 - acc: 0.99 - ETA: 9s - loss: 0.0279 - acc: 0.9940 - ETA: 9s - loss: 0.0285 - acc: 0.994 - ETA: 9s - loss: 0.0283 - acc: 0.994 - ETA: 9s - loss: 0.0288 - acc: 0.993 - ETA: 8s - loss: 0.0291 - acc: 0.993 - ETA: 8s - loss: 0.0292 - acc: 0.993 - ETA: 8s - loss: 0.0294 - acc: 0.993 - ETA: 8s - loss: 0.0295 - acc: 0.993 - ETA: 7s - loss: 0.0297 - acc: 0.993 - ETA: 7s - loss: 0.0301 - acc: 0.993 - ETA: 7s - loss: 0.0301 - acc: 0.993 - ETA: 7s - loss: 0.0300 - acc: 0.993 - ETA: 6s - loss: 0.0300 - acc: 0.993 - ETA: 6s - loss: 0.0309 - acc: 0.993 - ETA: 6s - loss: 0.0312 - acc: 0.993 - ETA: 6s - loss: 0.0314 - acc: 0.993 - ETA: 5s - loss: 0.0314 - acc: 0.993 - ETA: 5s - loss: 0.0319 - acc: 0.993 - ETA: 5s - loss: 0.0317 - acc: 0.993 - ETA: 4s - loss: 0.0315 - acc: 0.993 - ETA: 4s - loss: 0.0313 - acc: 0.993 - ETA: 4s - loss: 0.0316 - acc: 0.993 - ETA: 4s - loss: 0.0315 - acc: 0.993 - ETA: 3s - loss: 0.0316 - acc: 0.993 - ETA: 3s - loss: 0.0315 - acc: 0.993 - ETA: 3s - loss: 0.0316 - acc: 0.993 - ETA: 3s - loss: 0.0318 - acc: 0.993 - ETA: 2s - loss: 0.0319 - acc: 0.992 - ETA: 2s - loss: 0.0321 - acc: 0.992 - ETA: 2s - loss: 0.0322 - acc: 0.992 - ETA: 1s - loss: 0.0320 - acc: 0.992 - ETA: 1s - loss: 0.0323 - acc: 0.992 - ETA: 1s - loss: 0.0323 - acc: 0.992 - ETA: 1s - loss: 0.0325 - acc: 0.992 - ETA: 0s - loss: 0.0324 - acc: 0.992 - ETA: 0s - loss: 0.0327 - acc: 0.992 - ETA: 0s - loss: 0.0328 - acc: 0.992 - ETA: 0s - loss: 0.0329 - acc: 0.992 - 16s 9ms/step - loss: 0.0328 - acc: 0.9926 - val_loss: 0.1095 - val_acc: 0.9706\n",
      "Epoch 14/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0228 - acc: 0.99 - ETA: 13s - loss: 0.0230 - acc: 0.99 - ETA: 13s - loss: 0.0232 - acc: 0.99 - ETA: 13s - loss: 0.0255 - acc: 0.99 - ETA: 12s - loss: 0.0266 - acc: 0.99 - ETA: 12s - loss: 0.0253 - acc: 0.99 - ETA: 12s - loss: 0.0249 - acc: 0.99 - ETA: 12s - loss: 0.0251 - acc: 0.99 - ETA: 11s - loss: 0.0248 - acc: 0.99 - ETA: 11s - loss: 0.0264 - acc: 0.99 - ETA: 11s - loss: 0.0275 - acc: 0.99 - ETA: 11s - loss: 0.0279 - acc: 0.99 - ETA: 10s - loss: 0.0277 - acc: 0.99 - ETA: 10s - loss: 0.0280 - acc: 0.99 - ETA: 10s - loss: 0.0272 - acc: 0.99 - ETA: 10s - loss: 0.0270 - acc: 0.99 - ETA: 9s - loss: 0.0272 - acc: 0.9935 - ETA: 9s - loss: 0.0272 - acc: 0.993 - ETA: 9s - loss: 0.0271 - acc: 0.993 - ETA: 8s - loss: 0.0275 - acc: 0.993 - ETA: 8s - loss: 0.0273 - acc: 0.993 - ETA: 8s - loss: 0.0274 - acc: 0.993 - ETA: 8s - loss: 0.0269 - acc: 0.993 - ETA: 7s - loss: 0.0269 - acc: 0.993 - ETA: 7s - loss: 0.0270 - acc: 0.993 - ETA: 7s - loss: 0.0272 - acc: 0.993 - ETA: 7s - loss: 0.0273 - acc: 0.993 - ETA: 6s - loss: 0.0275 - acc: 0.993 - ETA: 6s - loss: 0.0273 - acc: 0.993 - ETA: 6s - loss: 0.0272 - acc: 0.993 - ETA: 5s - loss: 0.0276 - acc: 0.993 - ETA: 5s - loss: 0.0278 - acc: 0.993 - ETA: 5s - loss: 0.0279 - acc: 0.993 - ETA: 5s - loss: 0.0279 - acc: 0.993 - ETA: 4s - loss: 0.0280 - acc: 0.993 - ETA: 4s - loss: 0.0278 - acc: 0.993 - ETA: 4s - loss: 0.0279 - acc: 0.993 - ETA: 4s - loss: 0.0280 - acc: 0.993 - ETA: 3s - loss: 0.0282 - acc: 0.993 - ETA: 3s - loss: 0.0281 - acc: 0.993 - ETA: 3s - loss: 0.0282 - acc: 0.993 - ETA: 3s - loss: 0.0283 - acc: 0.993 - ETA: 2s - loss: 0.0285 - acc: 0.993 - ETA: 2s - loss: 0.0290 - acc: 0.993 - ETA: 2s - loss: 0.0289 - acc: 0.993 - ETA: 1s - loss: 0.0288 - acc: 0.993 - ETA: 1s - loss: 0.0290 - acc: 0.993 - ETA: 1s - loss: 0.0292 - acc: 0.993 - ETA: 1s - loss: 0.0293 - acc: 0.993 - ETA: 0s - loss: 0.0292 - acc: 0.993 - ETA: 0s - loss: 0.0291 - acc: 0.993 - ETA: 0s - loss: 0.0291 - acc: 0.993 - ETA: 0s - loss: 0.0291 - acc: 0.993 - 16s 9ms/step - loss: 0.0293 - acc: 0.9933 - val_loss: 0.1082 - val_acc: 0.9711\n",
      "Epoch 15/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0329 - acc: 0.99 - ETA: 13s - loss: 0.0293 - acc: 0.99 - ETA: 13s - loss: 0.0251 - acc: 0.99 - ETA: 13s - loss: 0.0276 - acc: 0.99 - ETA: 13s - loss: 0.0263 - acc: 0.99 - ETA: 13s - loss: 0.0291 - acc: 0.99 - ETA: 12s - loss: 0.0291 - acc: 0.99 - ETA: 12s - loss: 0.0288 - acc: 0.99 - ETA: 12s - loss: 0.0279 - acc: 0.99 - ETA: 11s - loss: 0.0281 - acc: 0.99 - ETA: 11s - loss: 0.0275 - acc: 0.99 - ETA: 11s - loss: 0.0278 - acc: 0.99 - ETA: 10s - loss: 0.0277 - acc: 0.99 - ETA: 10s - loss: 0.0268 - acc: 0.99 - ETA: 10s - loss: 0.0268 - acc: 0.99 - ETA: 10s - loss: 0.0267 - acc: 0.99 - ETA: 9s - loss: 0.0260 - acc: 0.9943 - ETA: 9s - loss: 0.0257 - acc: 0.994 - ETA: 9s - loss: 0.0251 - acc: 0.994 - ETA: 8s - loss: 0.0251 - acc: 0.994 - ETA: 8s - loss: 0.0254 - acc: 0.994 - ETA: 8s - loss: 0.0251 - acc: 0.994 - ETA: 8s - loss: 0.0254 - acc: 0.994 - ETA: 7s - loss: 0.0252 - acc: 0.994 - ETA: 7s - loss: 0.0253 - acc: 0.994 - ETA: 7s - loss: 0.0250 - acc: 0.994 - ETA: 7s - loss: 0.0255 - acc: 0.994 - ETA: 6s - loss: 0.0264 - acc: 0.994 - ETA: 6s - loss: 0.0264 - acc: 0.994 - ETA: 6s - loss: 0.0262 - acc: 0.994 - ETA: 5s - loss: 0.0261 - acc: 0.994 - ETA: 5s - loss: 0.0261 - acc: 0.994 - ETA: 5s - loss: 0.0261 - acc: 0.994 - ETA: 5s - loss: 0.0261 - acc: 0.994 - ETA: 4s - loss: 0.0263 - acc: 0.994 - ETA: 4s - loss: 0.0262 - acc: 0.994 - ETA: 4s - loss: 0.0264 - acc: 0.994 - ETA: 4s - loss: 0.0266 - acc: 0.994 - ETA: 3s - loss: 0.0266 - acc: 0.994 - ETA: 3s - loss: 0.0269 - acc: 0.994 - ETA: 3s - loss: 0.0268 - acc: 0.994 - ETA: 3s - loss: 0.0269 - acc: 0.994 - ETA: 2s - loss: 0.0268 - acc: 0.994 - ETA: 2s - loss: 0.0268 - acc: 0.994 - ETA: 2s - loss: 0.0268 - acc: 0.994 - ETA: 1s - loss: 0.0267 - acc: 0.994 - ETA: 1s - loss: 0.0266 - acc: 0.994 - ETA: 1s - loss: 0.0265 - acc: 0.994 - ETA: 1s - loss: 0.0266 - acc: 0.994 - ETA: 0s - loss: 0.0265 - acc: 0.994 - ETA: 0s - loss: 0.0264 - acc: 0.994 - ETA: 0s - loss: 0.0264 - acc: 0.994 - ETA: 0s - loss: 0.0265 - acc: 0.994 - 16s 9ms/step - loss: 0.0265 - acc: 0.9941 - val_loss: 0.1082 - val_acc: 0.9711\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0253 - acc: 0.99 - ETA: 13s - loss: 0.0197 - acc: 0.99 - ETA: 13s - loss: 0.0212 - acc: 0.99 - ETA: 12s - loss: 0.0210 - acc: 0.99 - ETA: 12s - loss: 0.0215 - acc: 0.99 - ETA: 12s - loss: 0.0208 - acc: 0.99 - ETA: 12s - loss: 0.0205 - acc: 0.99 - ETA: 11s - loss: 0.0207 - acc: 0.99 - ETA: 11s - loss: 0.0204 - acc: 0.99 - ETA: 11s - loss: 0.0202 - acc: 0.99 - ETA: 11s - loss: 0.0197 - acc: 0.99 - ETA: 10s - loss: 0.0194 - acc: 0.99 - ETA: 10s - loss: 0.0189 - acc: 0.99 - ETA: 10s - loss: 0.0193 - acc: 0.99 - ETA: 10s - loss: 0.0195 - acc: 0.99 - ETA: 9s - loss: 0.0194 - acc: 0.9961 - ETA: 9s - loss: 0.0197 - acc: 0.995 - ETA: 9s - loss: 0.0201 - acc: 0.995 - ETA: 9s - loss: 0.0199 - acc: 0.995 - ETA: 8s - loss: 0.0204 - acc: 0.995 - ETA: 8s - loss: 0.0201 - acc: 0.995 - ETA: 8s - loss: 0.0203 - acc: 0.995 - ETA: 8s - loss: 0.0206 - acc: 0.995 - ETA: 7s - loss: 0.0214 - acc: 0.995 - ETA: 7s - loss: 0.0215 - acc: 0.995 - ETA: 7s - loss: 0.0216 - acc: 0.995 - ETA: 7s - loss: 0.0221 - acc: 0.995 - ETA: 6s - loss: 0.0225 - acc: 0.995 - ETA: 6s - loss: 0.0222 - acc: 0.995 - ETA: 6s - loss: 0.0225 - acc: 0.995 - ETA: 6s - loss: 0.0228 - acc: 0.995 - ETA: 5s - loss: 0.0227 - acc: 0.995 - ETA: 5s - loss: 0.0225 - acc: 0.995 - ETA: 5s - loss: 0.0223 - acc: 0.995 - ETA: 4s - loss: 0.0223 - acc: 0.995 - ETA: 4s - loss: 0.0228 - acc: 0.995 - ETA: 4s - loss: 0.0229 - acc: 0.995 - ETA: 4s - loss: 0.0233 - acc: 0.994 - ETA: 3s - loss: 0.0233 - acc: 0.995 - ETA: 3s - loss: 0.0239 - acc: 0.994 - ETA: 3s - loss: 0.0237 - acc: 0.995 - ETA: 3s - loss: 0.0238 - acc: 0.994 - ETA: 2s - loss: 0.0239 - acc: 0.994 - ETA: 2s - loss: 0.0238 - acc: 0.994 - ETA: 2s - loss: 0.0238 - acc: 0.994 - ETA: 1s - loss: 0.0241 - acc: 0.994 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 1s - loss: 0.0240 - acc: 0.994 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 0s - loss: 0.0239 - acc: 0.994 - ETA: 0s - loss: 0.0238 - acc: 0.995 - ETA: 0s - loss: 0.0239 - acc: 0.994 - ETA: 0s - loss: 0.0240 - acc: 0.994 - 16s 9ms/step - loss: 0.0240 - acc: 0.9949 - val_loss: 0.1085 - val_acc: 0.9713\n",
      "Epoch 17/20\n",
      "1709/1709 [==============================] - ETA: 13s - loss: 0.0203 - acc: 0.99 - ETA: 13s - loss: 0.0189 - acc: 0.99 - ETA: 13s - loss: 0.0251 - acc: 0.99 - ETA: 12s - loss: 0.0251 - acc: 0.99 - ETA: 12s - loss: 0.0242 - acc: 0.99 - ETA: 12s - loss: 0.0229 - acc: 0.99 - ETA: 12s - loss: 0.0223 - acc: 0.99 - ETA: 12s - loss: 0.0233 - acc: 0.99 - ETA: 11s - loss: 0.0231 - acc: 0.99 - ETA: 11s - loss: 0.0244 - acc: 0.99 - ETA: 11s - loss: 0.0238 - acc: 0.99 - ETA: 10s - loss: 0.0232 - acc: 0.99 - ETA: 10s - loss: 0.0226 - acc: 0.99 - ETA: 10s - loss: 0.0226 - acc: 0.99 - ETA: 10s - loss: 0.0220 - acc: 0.99 - ETA: 9s - loss: 0.0217 - acc: 0.9954 - ETA: 9s - loss: 0.0215 - acc: 0.995 - ETA: 9s - loss: 0.0225 - acc: 0.995 - ETA: 9s - loss: 0.0226 - acc: 0.995 - ETA: 8s - loss: 0.0226 - acc: 0.995 - ETA: 8s - loss: 0.0222 - acc: 0.995 - ETA: 8s - loss: 0.0224 - acc: 0.995 - ETA: 8s - loss: 0.0222 - acc: 0.995 - ETA: 7s - loss: 0.0222 - acc: 0.995 - ETA: 7s - loss: 0.0220 - acc: 0.995 - ETA: 7s - loss: 0.0217 - acc: 0.995 - ETA: 7s - loss: 0.0217 - acc: 0.995 - ETA: 6s - loss: 0.0213 - acc: 0.995 - ETA: 6s - loss: 0.0217 - acc: 0.995 - ETA: 6s - loss: 0.0217 - acc: 0.995 - ETA: 6s - loss: 0.0216 - acc: 0.995 - ETA: 5s - loss: 0.0216 - acc: 0.995 - ETA: 5s - loss: 0.0216 - acc: 0.995 - ETA: 5s - loss: 0.0217 - acc: 0.995 - ETA: 4s - loss: 0.0218 - acc: 0.995 - ETA: 4s - loss: 0.0216 - acc: 0.995 - ETA: 4s - loss: 0.0215 - acc: 0.995 - ETA: 4s - loss: 0.0214 - acc: 0.995 - ETA: 3s - loss: 0.0214 - acc: 0.995 - ETA: 3s - loss: 0.0213 - acc: 0.995 - ETA: 3s - loss: 0.0212 - acc: 0.995 - ETA: 3s - loss: 0.0212 - acc: 0.995 - ETA: 2s - loss: 0.0212 - acc: 0.995 - ETA: 2s - loss: 0.0211 - acc: 0.995 - ETA: 2s - loss: 0.0213 - acc: 0.995 - ETA: 1s - loss: 0.0212 - acc: 0.995 - ETA: 1s - loss: 0.0213 - acc: 0.995 - ETA: 1s - loss: 0.0214 - acc: 0.995 - ETA: 1s - loss: 0.0213 - acc: 0.995 - ETA: 0s - loss: 0.0215 - acc: 0.995 - ETA: 0s - loss: 0.0214 - acc: 0.995 - ETA: 0s - loss: 0.0215 - acc: 0.995 - ETA: 0s - loss: 0.0215 - acc: 0.995 - 16s 9ms/step - loss: 0.0216 - acc: 0.9955 - val_loss: 0.1084 - val_acc: 0.9712\n"
     ]
    }
   ],
   "source": [
    "history = NER.fit(X, np.array(y), batch_size=32, epochs=20, validation_split=0.3,callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_set = read_dataset(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into numerical form\n",
    "X  =[]\n",
    "\n",
    "for idx,d_elem in test_set.iterrows():\n",
    "    seq = []\n",
    "    for w in d_elem[0]:\n",
    "        try:\n",
    "            num = word2idx[w]\n",
    "        except:\n",
    "            num = word2idx[\"ENDPAD\"]\n",
    "        finally:\n",
    "            seq.append(num)\n",
    "    X.append(seq)\n",
    "\n",
    "y = [[tag2idx[t] for t in d_elem[1]] for idx,d_elem in test_set.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(maxlen=max_seq_lenth, sequences=X, padding='post', value=word2idx[\"ENDPAD\"])\n",
    "y = pad_sequences(maxlen=max_seq_lenth, sequences=y, padding='post', value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encoding\n",
    "y = to_categorical(y, num_classes=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "best_model = load_model(\"best_model_NER.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - ETA: 1: - ETA: 51s - ETA: 35 - ETA: 27 - ETA: 22 - ETA: 19 - ETA: 17 - ETA: 15 - ETA: 14 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 7s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "score, acc = best_model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Acc: 0.9882464681269203\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Set Acc:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"ENDPAD\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag = dict(map(reversed, tag2idx.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 85.5%\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          TITLE       0.73      0.69      0.71       562\n",
      "RATINGS_AVERAGE       0.84      0.85      0.84       451\n",
      "       DIRECTOR       0.79      0.77      0.78       456\n",
      "           YEAR       0.94      0.97      0.96       720\n",
      "           PLOT       0.76      0.75      0.75       491\n",
      "         RATING       0.94      0.95      0.94       500\n",
      "          ACTOR       0.81      0.84      0.82       812\n",
      "          GENRE       0.95      0.96      0.95      1117\n",
      "         REVIEW       0.66      0.70      0.68        56\n",
      "        TRAILER       0.81      0.87      0.84        30\n",
      "      CHARACTER       0.71      0.67      0.69        89\n",
      "           SONG       0.69      0.74      0.71        54\n",
      "\n",
      "      micro avg       0.85      0.86      0.86      5338\n",
      "      macro avg       0.85      0.86      0.85      5338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
